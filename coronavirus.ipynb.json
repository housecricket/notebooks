{
  "cells": [
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Let assume that you are a doctor, you evaluating data for ten people and predicting if somebody could get coronavirus.\nIn our example. You predict six people will get coronavirus."
    },
    {
      "metadata": { "trusted": true },
      "cell_type": "code",
      "source": "y_pred = [0, 0, 0, 0, 1, 1, 1, 1, 1, 1]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "By the end of season, you find only five people had coronavirus."
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "y_true = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Building a confusion matrix."
    },
    {
      "metadata": { "trusted": true },
      "cell_type": "code",
      "source": "from sklearn.metrics import confusion_matrix\n\ntn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Disply the confusion matrix."
    },
    {
      "metadata": { "trusted": true },
      "cell_type": "code",
      "source": "print('Here is our confusion matrix.')\n\nimport pandas as pd\ncmtx = pd.DataFrame(\n    confusion_matrix(y_true, y_pred, labels=[1, 0]), \n    index=['true:1', 'true:0'], \n    columns=['pred:1', 'pred:0']\n)\nprint(cmtx)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We calculate the percentage of sick people who are correctly identified as having the condition (also called sensitivity)."
    },
    {
      "metadata": { "trusted": true },
      "cell_type": "code",
      "source": "sensitivity = tp / (tp+fn)\nprint('The percentage of sick people who are correctly identified as having the condition')\nprint('Sensitivity : %7.3f %%' % (sensitivity*100),'\\n')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We also the percentage of healthy people who are correctly identified as not having the condition (also called specificity)."
    },
    {
      "metadata": { "trusted": true },
      "cell_type": "code",
      "source": "specificity = tn / (tn+fp)\nprint('The percentage of healthy people who are correctly identified as not having the condition')\nprint('Specificity : %7.3f %%' % (specificity*100))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": { "trusted": true },
      "cell_type": "code",
      "source": "Next, we calculate the precision of this algorithm.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": { "trusted": true },
      "cell_type": "code",
      "source": "from sklearn.metrics import precision_score\nprint('The ratio of properly predicted positive clarifications to the total predicted positive clarifications.')\nprint(precision_score(y_true, y_pred, average=None))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "trusted": true
      },
      "cell_type": "code",
      "source": "npv = tn / (tn+fn)\nprint('The probability that records with a negative predicted result truly should be negative: %7.3f %%' % (npv*100))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We calculate the proportion of positives that yield negative prediction outcomes with the specific model (also called miss rate or FNR)."
    },
    {
      "metadata": { "trusted": true },
      "cell_type": "code",
      "source": "fnr = fp / (fn+tp)\nprint('The proportion of positives that yield negative prediction outcomes with the specific model: %7.3f %%' % (fnr*100))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "Finally, we calculate the false positive rate (also called FPR)."
    },
    {
      "metadata": { "trusted": true },
      "cell_type": "code",
      "source": "fdr = fp / (fp+tp)\nprint('False discovery rate: %7.3f %%' % (fdr*100))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "We calculate statistical bias, as these cause a difference between a result and a \"true\" value.\n"
    },
    {
      "metadata": { "trusted": true },
      "cell_type": "code",
      "source": "acc = (tp + tn) / (tp + tn + fp + fn)\nprint('Accuracy: %7.3f %%' % (acc*100))",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": { "name": "ipython", "version": 3 },
      "name": "python",
      "mimetype": "text/x-python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
